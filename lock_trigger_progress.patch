diff --git a/src/mercury_core.c b/src/mercury_core.c
index 9c42b78..ca55fbd 100644
--- a/src/mercury_core.c
+++ b/src/mercury_core.c
@@ -938,7 +938,7 @@ hg_core_trigger_lookup_entry(struct hg_core_op_id *hg_core_op_id);
  * Trigger callback from HG core handle.
  */
 static hg_return_t
-hg_core_trigger_entry(struct hg_core_private_handle *hg_core_handle);
+hg_core_trigger_entry(struct hg_core_private_handle *hg_core_handle, hg_thread_mutex_t *mutex);
 
 /**
  * Cancel handle.
@@ -4913,6 +4913,7 @@ hg_core_progress(
         hg_time_get_current_ms(&now);
     deadline = hg_time_add(now, hg_time_from_ms(timeout_ms));
 
+    hg_thread_mutex_lock(&context->loopback_notify.mutex);
     do {
         hg_bool_t safe_wait = HG_FALSE, progressed = HG_FALSE;
         unsigned int poll_timeout = 0;
@@ -4922,8 +4923,6 @@ hg_core_progress(
         if (timeout_ms == 0) {
             ; // nothing to do
         } else if (context->poll_set) {
-            hg_thread_mutex_lock(&context->loopback_notify.mutex);
-
             if (hg_core_poll_try_wait(context)) {
                 safe_wait = HG_TRUE;
                 poll_timeout = hg_time_to_ms(hg_time_subtract(deadline, now));
@@ -4931,7 +4930,6 @@ hg_core_progress(
                 /* We need to be notified when doing blocking progress */
                 hg_atomic_set32(&context->loopback_notify.must_notify, 1);
             }
-            hg_thread_mutex_unlock(&context->loopback_notify.mutex);
         } else if (!HG_CORE_CONTEXT_CLASS(context)->init_info.loopback &&
                    hg_core_poll_try_wait(context)) {
             /* This is the case for NA plugins that don't expose a fd */
@@ -4952,13 +4950,15 @@ hg_core_progress(
         /* We progressed or we have something to trigger */
         if (progressed ||
             !hg_atomic_queue_is_empty(context->completion_queue) ||
-            hg_atomic_get32(&context->backfill_queue.count) > 0)
-            return HG_SUCCESS;
+            hg_atomic_get32(&context->backfill_queue.count) > 0) {
+		hg_thread_mutex_unlock(&context->loopback_notify.mutex);
+		return HG_SUCCESS;
+	}
 
         if (timeout_ms != 0)
             hg_time_get_current_ms(&now);
     } while (hg_time_less(now, deadline));
-
+    hg_thread_mutex_unlock(&context->loopback_notify.mutex);
     return HG_TIMEOUT;
 
 error:
@@ -5199,25 +5199,24 @@ hg_core_trigger(struct hg_core_private_context *context,
     unsigned int count = 0;
     hg_return_t ret = HG_SUCCESS;
 
+    struct hg_core_completion_queue *backfill_queue =
+	    &context->backfill_queue;
+
     if (timeout_ms != 0)
         hg_time_get_current_ms(&now);
     deadline = hg_time_add(now, hg_time_from_ms(timeout_ms));
 
+    hg_thread_mutex_lock(&backfill_queue->mutex);
     while (count < max_count) {
         struct hg_completion_entry *hg_completion_entry = NULL;
 
         hg_completion_entry = hg_atomic_queue_pop_mc(context->completion_queue);
         if (!hg_completion_entry) {
-            struct hg_core_completion_queue *backfill_queue =
-                &context->backfill_queue;
-
             /* Check backfill queue */
             if (hg_atomic_get32(&backfill_queue->count) > 0) {
-                hg_thread_mutex_lock(&backfill_queue->mutex);
                 hg_completion_entry = HG_QUEUE_FIRST(&backfill_queue->queue);
                 HG_QUEUE_POP_HEAD(&backfill_queue->queue, entry);
                 hg_atomic_decr32(&backfill_queue->count);
-                hg_thread_mutex_unlock(&backfill_queue->mutex);
                 if (hg_completion_entry == NULL)
                     continue; /* Give another change to grab it */
             } else {
@@ -5231,7 +5230,6 @@ hg_core_trigger(struct hg_core_private_context *context,
                     break;
                 }
 
-                hg_thread_mutex_lock(&backfill_queue->mutex);
                 /* Otherwise wait remaining ms */
                 if (hg_atomic_queue_is_empty(context->completion_queue) &&
                     hg_atomic_get32(&backfill_queue->count) == 0) {
@@ -5241,7 +5239,6 @@ hg_core_trigger(struct hg_core_private_context *context,
                         HG_UTIL_SUCCESS)
                         ret = HG_TIMEOUT; /* Timeout occurred so leave */
                 }
-                hg_thread_mutex_unlock(&backfill_queue->mutex);
                 if (ret == HG_TIMEOUT)
                     break;
 
@@ -5258,21 +5255,25 @@ hg_core_trigger(struct hg_core_private_context *context,
         /* Trigger entry */
         switch (hg_completion_entry->op_type) {
             case HG_ADDR:
+                hg_thread_mutex_unlock(&backfill_queue->mutex);
                 ret = hg_core_trigger_lookup_entry(
                     hg_completion_entry->op_id.hg_core_op_id);
+		hg_thread_mutex_lock(&backfill_queue->mutex);
                 HG_CHECK_SUBSYS_HG_ERROR(
                     poll, done, ret, "Could not trigger addr completion entry");
                 break;
             case HG_RPC:
                 ret = hg_core_trigger_entry(
                     (struct hg_core_private_handle *)
-                        hg_completion_entry->op_id.hg_core_handle);
+                        hg_completion_entry->op_id.hg_core_handle, &backfill_queue->mutex);
                 HG_CHECK_SUBSYS_HG_ERROR(
                     poll, done, ret, "Could not trigger RPC completion entry");
                 break;
             case HG_BULK:
+                hg_thread_mutex_unlock(&backfill_queue->mutex);
                 ret = hg_bulk_trigger_entry(
                     hg_completion_entry->op_id.hg_bulk_op_id);
+		hg_thread_mutex_lock(&backfill_queue->mutex);
                 HG_CHECK_SUBSYS_HG_ERROR(
                     poll, done, ret, "Could not trigger bulk completion entry");
                 break;
@@ -5281,14 +5282,13 @@ hg_core_trigger(struct hg_core_private_context *context,
                     "Invalid type of completion entry (%d)",
                     (int) hg_completion_entry->op_type);
         }
-
         count++;
     }
-
     if (actual_count_p)
         *actual_count_p = count;
 
 done:
+    hg_thread_mutex_unlock(&backfill_queue->mutex);
     return ret;
 }
 
@@ -5317,7 +5317,7 @@ hg_core_trigger_lookup_entry(struct hg_core_op_id *hg_core_op_id)
 
 /*---------------------------------------------------------------------------*/
 static hg_return_t
-hg_core_trigger_entry(struct hg_core_private_handle *hg_core_handle)
+hg_core_trigger_entry(struct hg_core_private_handle *hg_core_handle, hg_thread_mutex_t *mutex)
 {
     hg_return_t ret;
 
@@ -5343,6 +5343,7 @@ hg_core_trigger_entry(struct hg_core_private_handle *hg_core_handle)
             hg_atomic_set32(&hg_core_handle->no_response_done, ref_count);
 
         /* Run RPC callback */
+	hg_thread_mutex_unlock(mutex);
         ret = hg_core_process(hg_core_handle);
         if (ret != HG_SUCCESS && !hg_core_handle->no_response) {
             hg_size_t header_size =
@@ -5352,15 +5353,20 @@ hg_core_trigger_entry(struct hg_core_private_handle *hg_core_handle)
             /* Respond in case of error */
             ret = hg_core_respond(
                 hg_core_handle, NULL, NULL, 0, header_size, ret);
+	    if (ret != HG_SUCCESS)
+		    hg_thread_mutex_lock(mutex);
             HG_CHECK_SUBSYS_HG_ERROR(rpc, done, ret, "Could not respond");
         }
 
         /* No response callback */
         if (hg_core_handle->no_response && !hg_core_handle->is_self) {
             ret = hg_core_handle->ops.no_respond(hg_core_handle);
+	    if (ret != HG_SUCCESS)
+		    hg_thread_mutex_lock(mutex);
             HG_CHECK_SUBSYS_HG_ERROR(
                 rpc, done, ret, "Could not complete handle");
         }
+	hg_thread_mutex_lock(mutex);
     } else {
         hg_core_cb_t hg_cb = NULL;
         struct hg_core_cb_info hg_core_cb_info;
@@ -5402,8 +5408,11 @@ hg_core_trigger_entry(struct hg_core_private_handle *hg_core_handle)
         /* Execute user callback.
          * NB. The handle cannot be destroyed before the callback execution
          * as the user may carry the handle in the callback. */
-        if (hg_cb)
+        if (hg_cb) {
+            hg_thread_mutex_unlock(mutex);
             hg_cb(&hg_core_cb_info);
+            hg_thread_mutex_lock(mutex);
+	}
     }
 
 done:
